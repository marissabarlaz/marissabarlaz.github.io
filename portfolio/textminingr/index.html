<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>Marissa Barlaz, PhD</title>
<meta name="description" content="Describe your website">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="/css/bootstrap.min.css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,300,700,400italic">
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/owl.carousel.css">
<link rel="stylesheet" href="/css/owl.theme.css">


  <link href="/css/style.default.css" rel="stylesheet" id="theme-stylesheet">

 

  
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  


<link href="/css/custom.css" rel="stylesheet">
<link rel="shortcut icon" href="/img/favicon.png">


</head>
<body>
  <div id="all">
      <div class="container-fluid">
          <div class="row row-offcanvas row-offcanvas-left">
              <div id="sidebar" class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas">
  <div class="sidebar-content">
    <h1 class="sidebar-heading"><a href="/">Marissa Barlaz, PhD</a></h1>
    
      <p class="sidebar-p">Until May 2020, I was the Linguistic Data Analytics Manager in the School of Literatures, Cultures, and Linguistics at the University of Illinois at Urbana-Champaign.</p>
    
    <ul class="sidebar-menu">
      
        <li><a href="/portfolio/">Home</a></li>
      
        <li><a href="/about/">About</a></li>
      
        <li><a href="/contact/">Get in touch</a></li>
      
    </ul>
    <p class="social">
  
  
  
  
  
  <a href="mailto:marissa.barlaz@gmail.com" data-animate-hover="pulse" class="email" title="E-mail">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://www.linkedin.com/in/marissa-barlaz-52801830/" data-animate-hover="pulse" class="external" title="LinkedIn">
    <i class="fa fa-linkedin"></i>
  </a>
  
  
  
  <a href="https://github.com/marissabarlaz" data-animate-hover="pulse" class="external" title="GitHub">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
</p>


    <div class="copyright">
      <p class="credit">
        
          &copy;2022 Marissa Barlaz |
        
        Template by <a href="https://bootstrapious.com/free-templates" class="external">Bootstrapious.com</a>

&amp; ported to Hugo by <a href="https://github.com/kishaningithub">Kishan B</a>

      </p>
    </div>
  </div>
</div>

              
<div class="col-xs-12 col-sm-8 col-md-9 content-column white-background">
  <div class="small-navbar visible-xs">
  <button type="button" data-toggle="offcanvas" class="btn btn-ghost pull-left"> <i class="fa fa-align-left"> </i>Menu</button>
  <h1 class="small-navbar-heading"><a href="/">Marissa Barlaz, PhD</a></h1>
</div>

  <div class="row">
    <div class="col-lg-8">
      <div class="content-column-content">
         <h1>Text Mining with R</h1>
         


<p>#Setting up an API
The first thing to do with R when getting ready to do Twitter mining is to set up your credentials. I won’t go through this process right now, but it is outlined <a href="https://github.com/mkearney/rtweet">here</a>. You need to first become a Twitter developer and create an app. It only takes a few moments.</p>
<p>There are a couple of different types of flows you can work with, depending on what your question is. For example, if you want to look at word counts and frequencies, your workflow will look something like this:</p>
<p><img src="/img/tidyflow-ch-1.png" style="width:95.0%" /></p>
<p>If you want to do sentiment analysis, your workflow will look something like this:</p>
<p><img src="/img/tidyflow-ch-2.png" style="width:95.0%" /></p>
<p>#Mining and Processing</p>
<p>##Searching for tweets</p>
<p>There are a number of functions/packages that allow you to search for tweets:</p>
<ul>
<li>searchTwitteR in the TwitteR package (I’ve had a lot of problems with this package and limitations on number of tweets allowed to be grabbed.)</li>
<li>search_tweets in the rtweet package</li>
<li>filterStream in the streamR package</li>
</ul>
<p>For this example, I will show you the results from the <em>rtweet</em> package. Note that these packages will only give recent tweets, generally from the past week. (Note, I collected these tweets on Sept. 5, 2018.)</p>
<pre class="r"><code>#This gets the 5000 most recent tweets from each of the handles listed.
timelines &lt;- get_timelines(c(&quot;nbcsports&quot;, &quot;cbssports&quot;, &quot;bbcsport&quot;), n = 5000, retryonratelimit = TRUE)

#This gets the 5000 most recent tweets that have one of the words listed. You can also use AND as a search operator to look for tweets that meet multiple criteria.
footballterms = search_tweets2(c(&quot;football OR  nfl OR superbowl&quot;, &quot;lang:en&quot;), n = 5000, retryonratelimit = TRUE, geocode = lookup_coords(&quot;usa&quot;))
footballterms = lat_lng(footballterms) #gives latitude and longitude for geocodes that are given

#searching for three types of cute animals
cuteanimals = search_tweets(&quot;dog OR cat OR hamster&quot;, n = 5000, retryonratelimit = TRUE, geocode = lookup_coords(&quot;usa&quot;))
cuteanimals = lat_lng(cuteanimals)
patterns = c(&quot;cat|dog|hamster&quot;)
#create a new variable that takes out which animal was mentioned
cuteanimals = cuteanimals %&gt;% mutate(animalmentioned = str_extract(tolower(text), patterns))


champaign = search_tweets(n = 10000, geocode =&quot;-88.24338, 40.11642, 5mi&quot;)
#save all files for future use

save(timelines, file = &quot;twittertimelines.Rda&quot;)
save(footballterms, file = &quot;footballterms.Rda&quot;)
save(cuteanimals, file = &quot;cuteanimals.Rda&quot;)</code></pre>
<p>Here’s how you would do it with the twitteR package. I personally have not had a lot of luck with this package, and I have heard it is being deprecated because of rtweets.</p>
<pre class="r"><code>#CBStweets = searchTwitteR(&quot;from:@cbssports&quot;, n = 1000)
#BBCtweets = searchTwitteR(&quot;from:@bbcsport&quot;, n = 1000)
#NBCtweets = searchTwitteR(&quot;from:@nbcsports&quot;, n = 1000)</code></pre>
<p>#Comparing three Twitter Users
##Processing Data</p>
<p>As you can see, there is a LOT of information we’re given when we download tweets. We only need a small subset of it, so we will use the tidyverse to select out the data we need, and we will forget the rest.</p>
<pre class="r"><code>head(timelines)</code></pre>
<pre><code>## # A tibble: 6 x 88
##   user_id status_id created_at          screen_name text  source
##   &lt;chr&gt;   &lt;chr&gt;     &lt;dttm&gt;              &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; 
## 1 118563… 10371523… 2018-09-05 01:36:04 NBCSports   Yasi… Spred…
## 2 118563… 10371448… 2018-09-05 01:06:05 NBCSports   Kobe… Spred…
## 3 118563… 10371440… 2018-09-05 01:03:02 NBCSports   Sere… Spred…
## 4 118563… 10371322… 2018-09-05 00:16:05 NBCSports   Is i… Spred…
## 5 118563… 10371246… 2018-09-04 23:46:04 NBCSports   Andr… Spred…
## 6 118563… 10371186… 2018-09-04 23:22:17 NBCSports   PHIL… Twitt…
## # … with 82 more variables: display_text_width &lt;dbl&gt;, reply_to_status_id &lt;chr&gt;,
## #   reply_to_user_id &lt;chr&gt;, reply_to_screen_name &lt;chr&gt;, is_quote &lt;lgl&gt;,
## #   is_retweet &lt;lgl&gt;, favorite_count &lt;int&gt;, retweet_count &lt;int&gt;,
## #   hashtags &lt;list&gt;, symbols &lt;chr&gt;, urls_url &lt;list&gt;, urls_t.co &lt;list&gt;,
## #   urls_expanded_url &lt;list&gt;, media_url &lt;list&gt;, media_t.co &lt;list&gt;,
## #   media_expanded_url &lt;list&gt;, media_type &lt;list&gt;, ext_media_url &lt;list&gt;,
## #   ext_media_t.co &lt;list&gt;, ext_media_expanded_url &lt;list&gt;, ext_media_type &lt;chr&gt;,
## #   mentions_user_id &lt;list&gt;, mentions_screen_name &lt;list&gt;, lang &lt;chr&gt;,
## #   quoted_status_id &lt;chr&gt;, quoted_text &lt;chr&gt;, quoted_created_at &lt;dttm&gt;,
## #   quoted_source &lt;chr&gt;, quoted_favorite_count &lt;int&gt;,
## #   quoted_retweet_count &lt;int&gt;, quoted_user_id &lt;chr&gt;, quoted_screen_name &lt;chr&gt;,
## #   quoted_name &lt;chr&gt;, quoted_followers_count &lt;int&gt;,
## #   quoted_friends_count &lt;int&gt;, quoted_statuses_count &lt;int&gt;,
## #   quoted_location &lt;chr&gt;, quoted_description &lt;chr&gt;, quoted_verified &lt;lgl&gt;,
## #   retweet_status_id &lt;chr&gt;, retweet_text &lt;chr&gt;, retweet_created_at &lt;dttm&gt;,
## #   retweet_source &lt;chr&gt;, retweet_favorite_count &lt;int&gt;,
## #   retweet_retweet_count &lt;int&gt;, retweet_user_id &lt;chr&gt;,
## #   retweet_screen_name &lt;chr&gt;, retweet_name &lt;chr&gt;,
## #   retweet_followers_count &lt;int&gt;, retweet_friends_count &lt;int&gt;,
## #   retweet_statuses_count &lt;int&gt;, retweet_location &lt;chr&gt;,
## #   retweet_description &lt;chr&gt;, retweet_verified &lt;lgl&gt;, place_url &lt;chr&gt;,
## #   place_name &lt;chr&gt;, place_full_name &lt;chr&gt;, place_type &lt;chr&gt;, country &lt;chr&gt;,
## #   country_code &lt;chr&gt;, geo_coords &lt;list&gt;, coords_coords &lt;list&gt;,
## #   bbox_coords &lt;list&gt;, status_url &lt;chr&gt;, name &lt;chr&gt;, location &lt;chr&gt;,
## #   description &lt;chr&gt;, url &lt;chr&gt;, protected &lt;lgl&gt;, followers_count &lt;int&gt;,
## #   friends_count &lt;int&gt;, listed_count &lt;int&gt;, statuses_count &lt;int&gt;,
## #   favourites_count &lt;int&gt;, account_created_at &lt;dttm&gt;, verified &lt;lgl&gt;,
## #   profile_url &lt;chr&gt;, profile_expanded_url &lt;chr&gt;, account_lang &lt;chr&gt;,
## #   profile_banner_url &lt;chr&gt;, profile_background_url &lt;chr&gt;,
## #   profile_image_url &lt;chr&gt;</code></pre>
<pre class="r"><code>dim(timelines)</code></pre>
<pre><code>## [1] 8827   88</code></pre>
<pre class="r"><code>timelines_processed = timelines %&gt;% select(screen_name, created_at, text, is_retweet, is_quote, favorite_count, retweet_count, hashtags)

timelines_processed %&gt;% group_by(screen_name) %&gt;% summarise(number = n())</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre><code>## # A tibble: 3 x 2
##   screen_name number
##   &lt;chr&gt;        &lt;int&gt;
## 1 BBCSport      2400
## 2 CBSSports     3227
## 3 NBCSports     3200</code></pre>
<p>##Word Ratios
Here, we are going to look at some words and see what the odds ratios are that these words belong to one user over another. This analysis is based on those found in <a href="www.tidytextmining.com">Tidy Text Mining with R</a>.</p>
<p>First, I have a list of symbols I would like to remove from any of the text. Those include ampersands, hash tags, at sign, etc. Then, we will also get rid of stop words and anything with numbers or other characers.</p>
<pre class="r"><code>remove_reg &lt;- &quot;#|&amp;|@|&lt;|&gt;&quot;

timelines_tidy_tweets &lt;- timelines_processed %&gt;% 
  filter(!str_detect(text, &quot;^RT&quot;)) %&gt;%
  mutate(text = str_remove_all(text, remove_reg)) %&gt;%
  unnest_tokens(word, text, token = &quot;tweets&quot;) %&gt;%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, &quot;&#39;&quot;),
         str_detect(word, &quot;[a-z]&quot;))</code></pre>
<pre><code>## Using `to_lower = TRUE` with `token = &#39;tweets&#39;` may not preserve URLs.</code></pre>
<p>Then, we are going to find the relative frequency for each word, by user name. We will find the log odds for each pairwise comparison of user names.</p>
<pre class="r"><code>timelines_frequency &lt;- timelines_tidy_tweets %&gt;% 
  group_by(screen_name) %&gt;% 
  count(word, sort = TRUE) %&gt;% 
  left_join(timelines_tidy_tweets %&gt;% 
              group_by(screen_name) %&gt;% 
              summarise(total = n())) %&gt;%
  mutate(freq = n/total)</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre><code>## Joining, by = &quot;screen_name&quot;</code></pre>
<pre class="r"><code>timelines_word_ratios &lt;- timelines_tidy_tweets %&gt;%
  filter(!str_detect(word, &quot;^@&quot;)) %&gt;%
  count(word, screen_name) %&gt;%
  group_by(word) %&gt;%
  filter(sum(n) &gt;= 10) %&gt;%
  ungroup() %&gt;%
  spread(screen_name, n, fill = 0) %&gt;%
  mutate_if(is.numeric, funs((. + 1) / (sum(.) + 1))) %&gt;%
  mutate(logratioBBCNBC = log(BBCSport / NBCSports), logratioCBSNBC = log(CBSSports / NBCSports),logratioBBCCBS = log(BBCSport / CBSSports))</code></pre>
<pre><code>## Warning: `funs()` is deprecated as of dplyr 0.8.0.
## Please use a list of either functions or lambdas: 
## 
##   # Simple named list: 
##   list(mean = mean, median = median)
## 
##   # Auto named with `tibble::lst()`: 
##   tibble::lst(mean, median)
## 
##   # Using lambdas
##   list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<p>And finally, we will visualize the results. Here, if the number is positive, it is more likely to be a word used of the first group than the second group. If the log ratio is negative, it is more likely to be a word used by the second group than the first group. I took the top 15 positive and negative log odds, ordered them, and then plotted them. I did this for all three pairs possible from this dataset.</p>
<pre class="r"><code>timelines_word_ratios %&gt;%
  group_by(logratioCBSNBC &lt; 0) %&gt;%
  top_n(15, abs(logratioCBSNBC)) %&gt;%
  ungroup() %&gt;%
  mutate(word = reorder(word, logratioCBSNBC)) %&gt;%
  ggplot(aes(word, logratioCBSNBC, fill = logratioCBSNBC &lt; 0)) +
  geom_col() +
  coord_flip() +
  ylab(&quot;log odds ratio (CBS/NBC)&quot;) +
  scale_fill_discrete(name = &quot;&quot;, labels = c(&quot;CBS&quot;, &quot;NBC&quot;))</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-8-1.png" width="576" /></p>
<pre class="r"><code>timelines_word_ratios %&gt;%
  group_by(logratioBBCNBC &lt; 0) %&gt;%
  top_n(15, abs(logratioBBCNBC)) %&gt;%
  ungroup() %&gt;%
  mutate(word = reorder(word, logratioBBCNBC)) %&gt;%
  ggplot(aes(word, logratioBBCNBC, fill = logratioBBCNBC &lt; 0)) +
  geom_col() +
  coord_flip() +
  ylab(&quot;log odds ratio (BBC/NBC)&quot;) +
  scale_fill_discrete(name = &quot;&quot;, labels = c(&quot;BBC&quot;, &quot;NBC&quot;))</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-8-2.png" width="576" /></p>
<pre class="r"><code>timelines_word_ratios %&gt;%
  group_by(logratioBBCCBS &lt; 0) %&gt;%
  top_n(15, abs(logratioBBCCBS)) %&gt;%
  ungroup() %&gt;%
  mutate(word = reorder(word, logratioBBCCBS)) %&gt;%
  ggplot(aes(word, logratioBBCCBS, fill = logratioBBCCBS &lt; 0)) +
  geom_col() +
  coord_flip() +
  ylab(&quot;log odds ratio (BBC/CBS)&quot;) +
  scale_fill_discrete(name = &quot;&quot;, labels = c(&quot;BBC&quot;, &quot;CBS&quot;))</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-8-3.png" width="576" /></p>
<p>##Frequencies of key words</p>
<p>Here, I have chosen a list of seven “key words” - that is, seven sports that are played in the US and UK. I filtered the list of frequencies for each word to only look for these words. Note that there are a few words that do not show up for a couple of the user names. Because these don’t show up, if we leave them out, the bar plot looks a bit weird.</p>
<pre class="r"><code>#IF there are 0s, they won&#39;t show up in the bar plot
sportslist = c(&quot;football&quot;, &quot;soccer&quot;, &quot;baseball&quot;, &quot;basketball&quot;, &quot;tennis&quot;, &quot;rugby&quot;, &quot;cricket&quot;)

frequentsports = timelines_frequency %&gt;% filter(word %in% sportslist)

dim(frequentsports)</code></pre>
<pre><code>## [1] 18  5</code></pre>
<pre class="r"><code>ggplot(frequentsports, aes(x = word, fill = screen_name)) + geom_bar(aes(weight = n), position = &quot;dodge&quot;)</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-9-1.png" width="576" /></p>
<p>If we want to include these specifically in the bar plot, even with values of 0, we need to make a new grid with all possible combinations of user_name and sport, and then merge this with the frequencies given from the tweets. Any word*screen_name pairs that do not exist will be filled in with a 0. This gives the bar plot we are looking for!</p>
<pre class="r"><code>#creating a full data grid with zeros to preserve shape of bar plot

sportslist = data.frame(word = rep(c(&quot;football&quot;, &quot;soccer&quot;, &quot;baseball&quot;, &quot;basketball&quot;, &quot;tennis&quot;, &quot;rugby&quot;, &quot;cricket&quot;), 3), screen_name = rep(c(&quot;BBCSport&quot;, &quot;NBCSports&quot;, &quot;CBSSports&quot;), each = 7))

frequentsports = timelines_frequency %&gt;% filter(word %in% sportslist$word)

fullsports_timelines = full_join(frequentsports, sportslist, by = c(&quot;screen_name&quot;, &quot;word&quot;), fill = 0)  %&gt;% replace(., is.na(.), 0)


ggplot(fullsports_timelines, aes(x = word, fill = screen_name)) + geom_bar(aes(weight = n), position = &quot;dodge&quot;)</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-10-1.png" width="576" /></p>
<p>##Sentiment analysis
In order to do sentiment analysis, we will use a few datasets from R’s tidytext package: the sentiments dataset, and the stop_words dataset, which comes with three general-purpose lexicons:</p>
<ul>
<li><em>FINN</em> from Finn Årup Nielsen,</li>
<li><em>bing</em> from Bing Liu and collaborators, and</li>
<li><em>nrc</em> from Saif Mohammad and Peter Turney.</li>
</ul>
<p>From the <a href="https://www.tidytextmining.com/sentiment.html">Text Mining with R</a> book (Section 2.1):</p>
<blockquote>
<p>All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words in a binary fashion into positive and negative categories. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. All of this information is tabulated in the sentiments dataset, and tidytext provides a function get_sentiments() to get specific sentiment lexicons without the columns that are not used in that lexicon.</p>
</blockquote>
<p>For the bing lexicon, a sentiment score is given by the number of times a word shows up in the corpus. We calculate sentiment = positive - negative, so that negative words have a negative score, and positive words have a positive score. We choose the top 30 words in terms of sentiment scores - that is, the 30 most frequent words that are related to sentiment.</p>
<pre class="r"><code>data(&quot;sentiments&quot;)

sports_sentiments_bing &lt;- timelines_frequency %&gt;%
  inner_join(get_sentiments(&quot;bing&quot;)) %&gt;%
  spread(sentiment, n,fill = 0) %&gt;%
  mutate(sentiment = positive - negative) %&gt;%
  top_n(30, abs(sentiment))</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>head(sports_sentiments_bing)</code></pre>
<pre><code>## # A tibble: 6 x 7
## # Groups:   screen_name [1]
##   screen_name word      total     freq negative positive sentiment
##   &lt;chr&gt;       &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 BBCSport    bad       27662 0.000542       15        0       -15
## 2 BBCSport    break     27662 0.000868       24        0       -24
## 3 BBCSport    brilliant 27662 0.000398        0       11        11
## 4 BBCSport    champion  27662 0.000651        0       18        18
## 5 BBCSport    defeat    27662 0.00119         0       33        33
## 6 BBCSport    defender  27662 0.000615        0       17        17</code></pre>
<pre class="r"><code>ggplot(sports_sentiments_bing, aes(word, sentiment, fill = sentiment&lt;0)) +  geom_col(show.legend = FALSE) +coord_flip()+  facet_wrap(~screen_name, ncol = 3, scales = &quot;free_x&quot;)</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-11-1.png" width="576" /></p>
<p>For the afinn sentiment dataset, we calculate sentiment by multiplying the score given by the dataset (ranging form -5 to 5) by the number of instances of that word. Once again, the top 30 words are chosen.</p>
<pre class="r"><code>sports_sentiments_afinn &lt;- timelines_frequency %&gt;%
  inner_join(tidytext::get_sentiments(&quot;afinn&quot;)) %&gt;% mutate(sentiment = n*value) %&gt;%
  top_n(30, abs(sentiment)) </code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>head(sports_sentiments_afinn)</code></pre>
<pre><code>## # A tibble: 6 x 7
## # Groups:   screen_name [3]
##   screen_name word      n total    freq value sentiment
##   &lt;chr&gt;       &lt;chr&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;
## 1 NBCSports   win     122 27818 0.00439     4       488
## 2 CBSSports   win      85 26080 0.00326     4       340
## 3 BBCSport    win      79 27662 0.00286     4       316
## 4 NBCSports   wins     64 27818 0.00230     4       256
## 5 BBCSport    top      58 27662 0.00210     2       116
## 6 CBSSports   block    54 26080 0.00207    -1       -54</code></pre>
<pre class="r"><code>ggplot(sports_sentiments_afinn, aes(word, sentiment, fill = sentiment&lt;0)) +  geom_col(show.legend = FALSE) +coord_flip()+  facet_wrap(~screen_name, ncol = 3, scales = &quot;free_x&quot;)</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-12-1.png" width="576" /></p>
<p>Here, we want to compare the counts per screen name for each sentiment word. The top 30 most frequent sentiment words, regardless of their positivity/negativity, are chosen.</p>
<pre class="r"><code>bing_word_counts =  timelines_tidy_tweets %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;%
  count(screen_name, word, sentiment, sort = TRUE) %&gt;%  ungroup()</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>head(bing_word_counts)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   screen_name word    sentiment     n
##   &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;
## 1 NBCSports   win     positive    122
## 2 BBCSport    premier positive    113
## 3 BBCSport    fans    positive     88
## 4 CBSSports   win     positive     85
## 5 BBCSport    win     positive     79
## 6 NBCSports   wins    positive     64</code></pre>
<pre class="r"><code>bing_word_counts %&gt;%
  group_by(screen_name, sentiment) %&gt;%
  top_n(30) %&gt;%
  ungroup() %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(word, n, fill = screen_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = &quot;free_y&quot;) +
  labs(y = &quot;Contribution to sentiment&quot;,
       x = NULL) +
  coord_flip()</code></pre>
<pre><code>## Selecting by n</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-13-1.png" width="576" /></p>
<p>And finally, we can create a word cloud using the top 100 words from the dataset.</p>
<pre class="r"><code>timelines_tidy_tweets %&gt;%
  anti_join(stop_words) %&gt;%
  group_by(screen_name) %&gt;%
  count(word) %&gt;%
  with(wordcloud(word, n, max.words = 100))</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre><code>## Warning in wordcloud(word, n, max.words = 100): england could not be fit on
## page. It will not be plotted.</code></pre>
<pre><code>## Warning in wordcloud(word, n, max.words = 100): stream could not be fit on page.
## It will not be plotted.</code></pre>
<pre><code>## Warning in wordcloud(word, n, max.words = 100): championship could not be fit on
## page. It will not be plotted.</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-14-1.png" width="576" /></p>
<p>#Comparing tweets with certain search terms
##Processing data
Now we will go on to searching for tweets that have certain words included. Once again, I processed the data to get rid of the extraneous information.</p>
<pre class="r"><code>footballterms_processed = footballterms %&gt;% select(screen_name, created_at, text, is_retweet, is_quote, favorite_count, retweet_count, hashtags, lat, lng)</code></pre>
<p>##Locations and time series
What if we want to see where/when tweets are being produced? I used previously the <em>lat_lng()</em> function from the rtweet package. This converts the geocoded information that is buried in the huge dataset into latitude and longitude. Note that I didn’t search specifically for geotagged tweets, so only a subset of my tweets have latitudes and longitudes. If you want to search for geotagged tweets, you can give <em>search_tweets()</em> a latitude, longitude and radius to geographically bound your search results, as such:</p>
<pre class="r"><code>#not run
rt &lt;- search_tweets(&quot;rstats&quot;, n = 500, include_rts = FALSE, geocode = &quot;37.78,-122.40,1mi&quot;)</code></pre>
<p>Here, I am first filtering my data so that I only include data with values for latitude and longitude. (I am filtering only on latitude, with the presumption that if I have a value for latitude, I will have one for longitude as well.)</p>
<p>I then am looking up the geocode for the United States, and calling to get a map from google maps. This is the basis of my geographical plot. The function <em>get_map</em> and the plotting function <em>ggmap</em> are found in the ggmap package. This allows you to use the map as a base for a ggplot-type figure, as I do below.</p>
<pre class="r"><code>head(footballterms_processed)</code></pre>
<pre><code>## # A tibble: 6 x 10
##   screen_name created_at          text  is_retweet is_quote favorite_count
##   &lt;chr&gt;       &lt;dttm&gt;              &lt;chr&gt; &lt;lgl&gt;      &lt;lgl&gt;             &lt;int&gt;
## 1 RampartapD  2018-09-05 14:22:19 &quot;Jus… TRUE       FALSE                 0
## 2 TDMindsTA   2018-09-05 14:22:19 &quot;@de… FALSE      FALSE                 0
## 3 leahcimekim 2018-09-05 14:22:19 &quot;@Br… FALSE      FALSE                 0
## 4 leahcimekim 2018-09-05 14:14:16 &quot;@Br… FALSE      FALSE                 0
## 5 leahcimekim 2018-09-05 14:13:50 &quot;@Br… FALSE      FALSE                 0
## 6 D_Valladar… 2018-09-05 14:22:19 &quot;4 D… TRUE       FALSE                 0
## # … with 4 more variables: retweet_count &lt;int&gt;, hashtags &lt;list&gt;, lat &lt;dbl&gt;,
## #   lng &lt;dbl&gt;</code></pre>
<pre class="r"><code>dim(footballterms_processed[!is.na(footballterms_processed$lat),])</code></pre>
<pre><code>## [1] 1075   10</code></pre>
<pre class="r"><code>footballgeo = footballterms_processed %&gt;% filter(!is.na(lat))
UScode = geocode(&quot;United States&quot;)</code></pre>
<pre><code>## Source : https://maps.googleapis.com/maps/api/geocode/json?address=United+States&amp;key=xxx</code></pre>
<pre class="r"><code>usmap&lt;-get_map(location=UScode, zoom=4, maptype = &quot;terrain&quot;, source = &quot;google&quot;)</code></pre>
<pre><code>## Source : https://maps.googleapis.com/maps/api/staticmap?center=37.09024,-95.712891&amp;zoom=4&amp;size=640x640&amp;scale=2&amp;maptype=terrain&amp;language=en-EN&amp;key=xxx</code></pre>
<pre class="r"><code>ggmap(usmap) + geom_point(data = footballgeo, aes(x = lng, y = lat, color = is_retweet))</code></pre>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_point).</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-17-1.png" width="576" /></p>
<p>Here’s another example of processing and tokenizing tweets. I am doing the same thing I did before for the timelines data - processing the strings, then getting frequencies.</p>
<p>Note that I am stripping the date and time from the <em>created_at</em> variable. The <em>created_at</em> variable is in a special date-time format for R, which makes plotting time series data very easy, so I don’t recommend you necessarily use the stripped data to create plots.</p>
<pre class="r"><code>data(&quot;stop_words&quot;)

remove_reg &lt;- &quot;@|&amp;|#&quot;
football_tweets &lt;- footballterms_processed %&gt;% 
  filter(is_retweet == FALSE) %&gt;%
  mutate(text = str_remove_all(text, remove_reg)) %&gt;%
  unnest_tokens(word, text, token = &quot;tweets&quot;) %&gt;%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, &quot;&#39;&quot;),
         str_detect(word, &quot;[a-z]&quot;),
         !str_detect(word, &quot;^http&quot;), 
         !word==&quot;amp&quot;)</code></pre>
<pre><code>## Using `to_lower = TRUE` with `token = &#39;tweets&#39;` may not preserve URLs.</code></pre>
<pre class="r"><code>head(football_tweets)</code></pre>
<pre><code>## # A tibble: 6 x 10
##   screen_name created_at          is_retweet is_quote favorite_count
##   &lt;chr&gt;       &lt;dttm&gt;              &lt;lgl&gt;      &lt;lgl&gt;             &lt;int&gt;
## 1 TDMindsTA   2018-09-05 14:22:19 FALSE      FALSE                 0
## 2 TDMindsTA   2018-09-05 14:22:19 FALSE      FALSE                 0
## 3 TDMindsTA   2018-09-05 14:22:19 FALSE      FALSE                 0
## 4 TDMindsTA   2018-09-05 14:22:19 FALSE      FALSE                 0
## 5 TDMindsTA   2018-09-05 14:22:19 FALSE      FALSE                 0
## 6 TDMindsTA   2018-09-05 14:22:19 FALSE      FALSE                 0
## # … with 5 more variables: retweet_count &lt;int&gt;, hashtags &lt;list&gt;, lat &lt;dbl&gt;,
## #   lng &lt;dbl&gt;, word &lt;chr&gt;</code></pre>
<pre class="r"><code>football_frequency &lt;- football_tweets %&gt;% 
#  group_by(screen_name) %&gt;% 
  count(word, sort = TRUE) %&gt;% 
  left_join(football_tweets) %&gt;%
  rename(number = n) %&gt;%
  mutate(freq = number/n(), date = strftime(created_at, format = &quot;%m/%d/%y&quot;), time = strftime(created_at, format=&quot;%H:%M:%S&quot;)) %&gt;% select(screen_name, created_at, word, number, freq, everything()) %&gt;% select(-hashtags) </code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<p>Here, I am plotting the top 20 most frequent words.</p>
<pre class="r"><code>football_frequency %&gt;% group_by(word) %&gt;% summarise(number = max(number)) %&gt;% arrange(desc(number)) %&gt;% top_n(20, number) %&gt;% ggplot(aes(reorder(word, number), number)) +geom_col() +coord_flip()</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-19-1.png" width="576" /></p>
<p>And here, I am doing a time-series plot using the <em>ts_plot</em> function from the rtweet package. First, I am looking at the data by hour, and then by minute. Because I collected this data from a small window of time, these aren’t particularly informative, but if you had a larger dataset, you could do some cool time series work. <a href="https://www.tidytextmining.com/twitter.html">Tidy Text Mining with R</a> gives another example of time-dynamic analyses.</p>
<pre class="r"><code>football10 = football_frequency %&gt;% group_by(word) %&gt;% summarise(number = max(number)) %&gt;% arrange(desc(number))%&gt;%top_n(10,number)%&gt;% left_join(football_frequency) %&gt;% group_by(word)</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre><code>## Joining, by = c(&quot;word&quot;, &quot;number&quot;)</code></pre>
<pre class="r"><code>ts_plot(football10, &quot;hours&quot;)</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-20-1.png" width="576" /></p>
<pre class="r"><code>ts_plot(football10, &quot;mins&quot;)</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-20-2.png" width="576" /></p>
<p>And finally, you can create another word cloud!</p>
<pre class="r"><code>football_frequency %&gt;%
  anti_join(stop_words)%&gt;%
  group_by(word) %&gt;% summarise(number = max(number)) %&gt;% top_n(100, number)%&gt;%
  with(wordcloud(word, number, max.words = 100))</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-21-1.png" width="576" /></p>
<p>#Another example of multiple search terms</p>
<p>Finally, I am looking at the relative distributions of dogs, cats and hamsters on Twitter. Note that I created a variable a while ago on my dataset, called <em>animalmentioned</em>, which pulled out which of my three search terms is mentioned in the tweet. I created the tokenized tweets file one more.
##Data processing</p>
<pre class="r"><code>animals_processed = cuteanimals %&gt;% select(screen_name, animalmentioned, created_at, text, is_retweet, is_quote, favorite_count, retweet_count, hashtags, lat, lng)

head(animals_processed)</code></pre>
<pre><code>## # A tibble: 6 x 11
##   screen_name animalmentioned created_at          text  is_retweet is_quote
##   &lt;chr&gt;       &lt;chr&gt;           &lt;dttm&gt;              &lt;chr&gt; &lt;lgl&gt;      &lt;lgl&gt;   
## 1 denisngahu  cat             2018-09-05 17:07:18 &quot;Rem… TRUE       FALSE   
## 2 AnimalAnge… cat             2018-09-05 17:07:18 &quot;Ple… TRUE       FALSE   
## 3 pecanacres  &lt;NA&gt;            2018-09-05 17:07:15 &quot;Fev… FALSE      FALSE   
## 4 RufnekBob   dog             2018-09-05 17:07:15 &quot;Dog… TRUE       FALSE   
## 5 JesionekSa… cat             2018-09-05 17:07:15 &quot;@Je… FALSE      FALSE   
## 6 badgaIzak   cat             2018-09-05 17:07:13 &quot;I j… TRUE       FALSE   
## # … with 5 more variables: favorite_count &lt;int&gt;, retweet_count &lt;int&gt;,
## #   hashtags &lt;list&gt;, lat &lt;dbl&gt;, lng &lt;dbl&gt;</code></pre>
<pre class="r"><code>  remove_reg &lt;- &quot;@|&amp;|#&quot;
animal_tweets &lt;- animals_processed %&gt;% 
  filter(is_retweet == FALSE) %&gt;%
  mutate(text = str_remove_all(text, remove_reg)) %&gt;%
  unnest_tokens(word, text, token = &quot;tweets&quot;) %&gt;%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, &quot;&#39;&quot;),
         str_detect(word, &quot;[a-z]&quot;),
         !str_detect(word, &quot;^http&quot;), 
         !word ==&quot;amp&quot;)</code></pre>
<pre><code>## Using `to_lower = TRUE` with `token = &#39;tweets&#39;` may not preserve URLs.</code></pre>
<p>##Time and Space
Here, I am looking at tweets over time and space by the animal mentioned. As you can see, not a lot of people are talking about hamsters!</p>
<pre class="r"><code>animalgeo = animals_processed %&gt;% filter(!is.na(lat), !is.na(animalmentioned))
ggmap(usmap) + geom_point(data = animalgeo, aes(x = lng, y = lat, color = animalmentioned))</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-23-1.png" width="576" /></p>
<p>Here, I am comparing the top 10 words used in reference to animals.</p>
<pre class="r"><code>animal_frequency &lt;- animal_tweets %&gt;% 
#  group_by(screen_name) %&gt;% 
  count(word, sort = TRUE) %&gt;% 
  left_join(animal_tweets) %&gt;%
  rename(number = n) %&gt;%
  mutate(freq = number/n(), date = strftime(created_at, format = &quot;%m/%d/%y&quot;), time = strftime(created_at, format=&quot;%H:%M:%S&quot;)) %&gt;% select(screen_name, created_at, word, number, freq, everything()) %&gt;% select(-hashtags) </code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>animal10 = animal_frequency %&gt;% group_by(word) %&gt;% summarise(number = max(number)) %&gt;% arrange(desc(number))%&gt;%top_n(10,number)%&gt;% left_join(animal_frequency) %&gt;% group_by(word)</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre><code>## Joining, by = c(&quot;word&quot;, &quot;number&quot;)</code></pre>
<pre class="r"><code>ts_plot(animal10, &quot;hours&quot;)</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-24-1.png" width="576" /></p>
<pre class="r"><code>ts_plot(animal10, &quot;mins&quot;)</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-24-2.png" width="576" /></p>
<p>##Sentiments
Let’s look at sentiments in reference to animals. Once again I will use the Bing sentiment corpus. I also have created another dataset, animals_bing_time, that looks at sentiments over time by dividing the timestamps into 40 windows.</p>
<pre class="r"><code>data(&quot;sentiments&quot;)

animals_bing &lt;- animal_tweets %&gt;%
  inner_join(get_sentiments(&quot;bing&quot;))%&gt;% 
  count(word, sentiment, animalmentioned, sort = TRUE) %&gt;% 
  spread(sentiment, n,fill = 0)%&gt;%
  mutate(sentiment = positive - negative) %&gt;%
  top_n(30, abs(sentiment))</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>animals_bing_time &lt;- animal_tweets %&gt;%
  inner_join(get_sentiments(&quot;bing&quot;))%&gt;% 
  count(index = as.numeric(cut(created_at, 40)), sentiment, sort = TRUE) %&gt;% 
  spread(sentiment, n,fill = 0)%&gt;%
  mutate(sentiment = positive - negative)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>ggplot(animals_bing, aes(word, sentiment, fill = sentiment&lt;0)) +
  geom_col(show.legend = FALSE) +coord_flip()+
  facet_wrap(~animalmentioned, ncol = 3, scales = &quot;free_x&quot;)</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-25-1.png" width="576" /></p>
<pre class="r"><code>ggplot(animals_bing_time, aes(index, sentiment, fill = sentiment&lt;0)) +
  geom_col(show.legend = FALSE) + scale_x_continuous(breaks=seq(0,40,10))</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-25-2.png" width="576" /></p>
<pre class="r"><code>animals_bing_time %&gt;%  top_n(50, abs(sentiment)) %&gt;%ggplot(aes(index, sentiment, fill = sentiment&lt;0)) +  geom_col(show.legend = FALSE) + scale_x_continuous(breaks=seq(0,40,10))</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-25-3.png" width="576" /></p>
<p>Again, I am using the afinn sentiment corpus.</p>
<pre class="r"><code>animals_sentiments_afinn &lt;- animal_frequency %&gt;%
  inner_join(get_sentiments(&quot;afinn&quot;)) %&gt;% mutate(sentiment = number*value, index = as.numeric(cut(created_at, 40))) %&gt;%count(word, sentiment, sort = TRUE)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>animals_sentiments_afinn   %&gt;% top_n(10) %&gt;%
  ggplot( aes(reorder(word, desc(sentiment)), sentiment, fill = sentiment&lt;0)) +
  geom_col(show.legend = FALSE) +coord_flip() + xlab(&quot;Word&quot;)</code></pre>
<pre><code>## Selecting by n</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-26-1.png" width="576" /></p>
<pre class="r"><code>animals_counts_nbing =  animal_tweets %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;%
  count(animalmentioned, word, sentiment, sort = TRUE) %&gt;%
  ungroup()</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>head(animals_counts_nbing)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   animalmentioned word  sentiment     n
##   &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;
## 1 dog             love  positive    106
## 2 dog             hot   positive     85
## 3 cat             love  positive     79
## 4 dog             shit  negative     53
## 5 dog             bad   negative     47
## 6 dog             cute  positive     45</code></pre>
<pre class="r"><code>animals_counts_nbing %&gt;%
  group_by(animalmentioned, sentiment) %&gt;%
  top_n(10) %&gt;%
  ungroup() %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(word, n, fill = animalmentioned)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = &quot;free_y&quot;) +
  labs(y = &quot;Contribution to sentiment&quot;,
       x = NULL) +
  coord_flip()</code></pre>
<pre><code>## Selecting by n</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-26-2.png" width="576" />
And a word cloud!</p>
<pre class="r"><code>animal_tweets %&gt;%
  anti_join(stop_words) %&gt;%
  count(word) %&gt;%
  with(wordcloud(word, n, max.words = 100))</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<p><img src="/portfolio/TextMiningR_files/figure-html/unnamed-chunk-27-1.png" width="576" /></p>
<p>#A few final words</p>
<p>Using the other packages I mentioned, as well as rtweet, you can do a lot - you can stream tweets, follow users, etc.
Here, I used a bounding box I determined interactively <a href="http://boundingbox.klokantech.com/">here</a> to stream tweets found in Chicago. (I tried Champaign, but we don’t seem to tweet much!)</p>
<pre class="r"><code>champaignbox = c(-88.290213,40.06123,-88.178074,40.166118)
chicagobox = c(-87.741984,41.790998,-87.528422,41.989486)
filterStream( file=&quot;tweets_rstats.json&quot;, locations=chicagobox, timeout=60, oauth=my_oauth )
tweets.df &lt;- parseTweets(&quot;tweets_rstats.json&quot;)</code></pre>

         
      </div>
    </div>
  </div>
</div>

          </div>
      </div>
  </div>
  <script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/jquery.cookie.js"> </script>
<script src="/js/ekko-lightbox.js"></script>
<script src="/js/jquery.scrollTo.min.js"></script>
<script src="/js/masonry.pkgd.min.js"></script>
<script src="/js/imagesloaded.pkgd.min.js"></script>
<script src="/js/owl.carousel.min.js"></script>
<script src="/js/front.js"></script>



</body>
</html>
